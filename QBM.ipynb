{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Quantum Restricted Boltzmann Machine Algorithm Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize, fmin_bfgs\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from pyquil import Program\n",
    "from pyquil.api import QuantumComputer, WavefunctionSimulator\n",
    "from pyquil.gates import H, MEASURE\n",
    "from pyquil.paulis import exponential_map, PauliSum\n",
    "from scipy import optimize, ufunc\n",
    "import funcsigs \n",
    "import pyquil.quil as pq\n",
    "from pyquil import get_qc, Program\n",
    "import pyquil.api as api\n",
    "from pyquil.paulis import *\n",
    "from pyquil.gates import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Variational Quantum Eigensolver from grove to perform VQE for the Variational QRBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptResults(dict):\n",
    "    \"\"\"\n",
    "    Object for holding optimization results from VQE.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(name)\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "class VQE(object):\n",
    "    \"\"\"\n",
    "    The Variational-Quantum-Eigensolver algorithm\n",
    "    VQE is an object that encapsulates the VQE algorithm (functional\n",
    "    minimization). The main components of the VQE algorithm are a minimizer\n",
    "    function for performing the functional minimization, a function that takes a\n",
    "    vector of parameters and returns a pyQuil program, and a\n",
    "    Hamiltonian of which to calculate the expectation value.\n",
    "    Using this object:\n",
    "        1) initialize with `inst = VQE(minimizer)` where `minimizer` is a\n",
    "        function that performs a gradient free minization--i.e\n",
    "        scipy.optimize.minimize(. , ., method='Nelder-Mead')\n",
    "        2) call `inst.vqe_run(variational_state_evolve, hamiltonian,\n",
    "        initial_parameters)`. Returns the optimal parameters and minimum\n",
    "        expecation\n",
    "    :param minimizer: function that minimizes objective f(obj, param). For\n",
    "                      example the function scipy.optimize.minimize() needs\n",
    "                      at least two parameters, the objective and an initial\n",
    "                      point for the optimization.  The args for minimizer\n",
    "                      are the cost function (provided by this class),\n",
    "                      initial parameters (passed to vqe_run() method, and\n",
    "                      jacobian (defaulted to None).  kwargs can be passed\n",
    "                      in below.\n",
    "    :param minimizer_args: (list) arguments for minimizer function. Default=None\n",
    "    :param minimizer_kwargs: (dict) arguments for keyword args.\n",
    "                              Default=None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, minimizer, minimizer_args=[], minimizer_kwargs={}):\n",
    "        self.minimizer = minimizer\n",
    "        self.minimizer_args = minimizer_args\n",
    "        self.minimizer_kwargs = minimizer_kwargs\n",
    "        self.n_qubits = None\n",
    "\n",
    "    def vqe_run(self, variational_state_evolve, hamiltonian, initial_params,\n",
    "                gate_noise=None, measurement_noise=None,\n",
    "                jacobian=None, qc=None, disp=None, samples=None, return_all=False):\n",
    "        \"\"\"\n",
    "        functional minimization loop.\n",
    "        :param variational_state_evolve: function that takes a set of parameters\n",
    "                                        and returns a pyQuil program.\n",
    "        :param hamiltonian: (PauliSum) object representing the hamiltonian of\n",
    "                            which to take the expectation value.\n",
    "        :param initial_params: (ndarray) vector of initial parameters for the\n",
    "                               optimization\n",
    "        :param gate_noise: list of Px, Py, Pz probabilities of gate being\n",
    "                           applied to every gate after each get application\n",
    "        :param measurement_noise: list of Px', Py', Pz' probabilities of a X, Y\n",
    "                                  or Z being applied before a measurement.\n",
    "        :param jacobian: (optional) method of generating jacobian for parameters\n",
    "                         (Default=None).\n",
    "        :param qc: (optional) QuantumComputer object.\n",
    "        :param disp: (optional, bool) display level. If True then each iteration\n",
    "                     expectation and parameters are printed at each\n",
    "                     optimization iteration.\n",
    "        :param samples: (int) Number of samples for calculating the expectation\n",
    "                        value of the operators.  If `None` then faster method\n",
    "                        ,dotting the wave function with the operator, is used.\n",
    "                        Default=None.\n",
    "        :param return_all: (optional, bool) request to return all intermediate\n",
    "                           parameters determined during the optimization.\n",
    "        :return: (vqe.OptResult()) object :func:`OptResult <vqe.OptResult>`.\n",
    "                 The following fields are initialized in OptResult:\n",
    "                 -x: set of w.f. ansatz parameters\n",
    "                 -fun: scalar value of the objective function\n",
    "                 -iteration_params: a list of all intermediate parameter vectors. Only\n",
    "                                    returned if 'return_all=True' is set as a vqe_run()\n",
    "                                    option.\n",
    "                 -expectation_vals: a list of all intermediate expectation values. Only\n",
    "                                    returned if 'return_all=True' is set as a\n",
    "                                    vqe_run() option.\n",
    "        \"\"\"\n",
    "        self._disp_fun = disp if disp is not None else lambda x: None\n",
    "        iteration_params = []\n",
    "        expectation_vals = []\n",
    "        self._current_expectation = None\n",
    "        if samples is None:\n",
    "            print(\"\"\"WARNING: Fast method for expectation will be used. Noise\n",
    "                     models will be ineffective\"\"\")\n",
    "\n",
    "        if qc is None:\n",
    "            qubits = hamiltonian.get_qubits()\n",
    "            qc = QuantumComputer(name=f\"{len(qubits)}q-noisy-qvm\",\n",
    "                                 qam=QVM(gate_noise=gate_noise,\n",
    "                                         measurement_noise=measurement_noise))\n",
    "        else:\n",
    "            self.qc = qc\n",
    "\n",
    "        def objective_function(params):\n",
    "            \"\"\"\n",
    "            closure representing the functional\n",
    "            :param params: (ndarray) vector of parameters for generating the\n",
    "                           the function of the functional.\n",
    "            :return: (float) expectation value\n",
    "            \"\"\"\n",
    "            pyquil_prog = variational_state_evolve(params)\n",
    "            mean_value = self.expectation(pyquil_prog, hamiltonian, samples, qc)\n",
    "            self._current_expectation = mean_value  # store for printing\n",
    "            return mean_value\n",
    "\n",
    "        def print_current_iter(iter_vars):\n",
    "            self._disp_fun(\"\\tParameters: {} \".format(iter_vars))\n",
    "            if jacobian is not None:\n",
    "                grad = jacobian(iter_vars)\n",
    "                self._disp_fun(\"\\tGrad-L1-Norm: {}\".format(np.max(np.abs(grad))))\n",
    "                self._disp_fun(\"\\tGrad-L2-Norm: {} \".format(np.linalg.norm(grad)))\n",
    "\n",
    "            self._disp_fun(\"\\tE => {}\".format(self._current_expectation))\n",
    "            if return_all:\n",
    "                iteration_params.append(iter_vars)\n",
    "                expectation_vals.append(self._current_expectation)\n",
    "\n",
    "        # using self.minimizer\n",
    "        arguments = funcsigs.signature(self.minimizer).parameters.keys()\n",
    "\n",
    "        if disp is not None and 'callback' in arguments:\n",
    "            self.minimizer_kwargs['callback'] = print_current_iter\n",
    "\n",
    "        args = [objective_function, initial_params]\n",
    "        args.extend(self.minimizer_args)\n",
    "        if 'jac' in arguments:\n",
    "            self.minimizer_kwargs['jac'] = jacobian\n",
    "\n",
    "        result = self.minimizer(*args, **self.minimizer_kwargs)\n",
    "\n",
    "        if hasattr(result, 'status'):\n",
    "            if result.status != 0:\n",
    "                self._disp_fun(\"Classical optimization exited with an error index: %i\"\n",
    "                               % result.status)\n",
    "\n",
    "        results = OptResults()\n",
    "        if hasattr(result, 'x'):\n",
    "            results.x = result.x\n",
    "            results.fun = result.fun\n",
    "        else:\n",
    "            results.x = result\n",
    "\n",
    "        if return_all:\n",
    "            results.iteration_params = iteration_params\n",
    "            results.expectation_vals = expectation_vals\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def expectation(pyquil_prog: Program,\n",
    "                    pauli_sum: Union[PauliSum, PauliTerm, np.ndarray],\n",
    "                    samples: int,\n",
    "                    qc: QuantumComputer) -> float:\n",
    "        \"\"\"\n",
    "        Compute the expectation value of pauli_sum over the distribution generated from pyquil_prog.\n",
    "        :param pyquil_prog: The state preparation Program to calculate the expectation value of.\n",
    "        :param pauli_sum: PauliSum representing the operator of which to calculate the expectation\n",
    "            value or a numpy matrix representing the Hamiltonian tensored up to the appropriate\n",
    "            size.\n",
    "        :param samples: The number of samples used to calculate the expectation value. If samples\n",
    "            is None then the expectation value is calculated by calculating <psi|O|psi>. Error\n",
    "            models will not work if samples is None.\n",
    "        :param qc: The QuantumComputer object.\n",
    "        :return: A float representing the expectation value of pauli_sum given the distribution\n",
    "            generated from quil_prog.\n",
    "        \"\"\"\n",
    "        if isinstance(pauli_sum, np.ndarray):\n",
    "            # debug mode by passing an array\n",
    "            wf = WavefunctionSimulator().wavefunction(pyquil_prog)\n",
    "            wf = np.reshape(wf.amplitudes, (-1, 1))\n",
    "            average_exp = np.conj(wf).T.dot(pauli_sum.dot(wf)).real\n",
    "            return average_exp\n",
    "        else:\n",
    "            if not isinstance(pauli_sum, (PauliTerm, PauliSum)):\n",
    "                raise TypeError(\"pauli_sum variable must be a PauliTerm or PauliSum object\")\n",
    "\n",
    "            if isinstance(pauli_sum, PauliTerm):\n",
    "                pauli_sum = PauliSum([pauli_sum])\n",
    "\n",
    "            if samples is None:\n",
    "                operator_progs = []\n",
    "                operator_coeffs = []\n",
    "                for p_term in pauli_sum.terms:\n",
    "                    op_prog = Program()\n",
    "                    for qindex, op in p_term:\n",
    "                        op_prog.inst(STANDARD_GATES[op](qindex))\n",
    "                    operator_progs.append(op_prog)\n",
    "                    operator_coeffs.append(p_term.coefficient)\n",
    "\n",
    "                result_overlaps = WavefunctionSimulator().expectation(pyquil_prog, pauli_sum.terms)\n",
    "                result_overlaps = list(result_overlaps)\n",
    "                assert len(result_overlaps) == len(operator_progs),\\\n",
    "                    \"\"\"Somehow we didn't get the correct number of results back from the QVM\"\"\"\n",
    "                expectation = sum(list(map(lambda x: x[0] * x[1],\n",
    "                                           zip(result_overlaps, operator_coeffs))))\n",
    "                return expectation.real\n",
    "            else:\n",
    "                if not isinstance(samples, int):\n",
    "                    raise TypeError(\"samples variable must be an integer\")\n",
    "                if samples <= 0:\n",
    "                    raise ValueError(\"samples variable must be a positive integer\")\n",
    "\n",
    "                # normal execution via fake sampling\n",
    "                # stores the sum of contributions to the energy from each operator term\n",
    "                expectation = 0.0\n",
    "                for j, term in enumerate(pauli_sum.terms):\n",
    "                    meas_basis_change = Program()\n",
    "                    qubits_to_measure = []\n",
    "                    if term.id() == \"\":\n",
    "                        meas_outcome = 1.0\n",
    "                    else:\n",
    "                        for index, gate in term:\n",
    "                            qubits_to_measure.append(index)\n",
    "                            if gate == 'X':\n",
    "                                meas_basis_change.inst(RY(-np.pi / 2, index))\n",
    "                            elif gate == 'Y':\n",
    "                                meas_basis_change.inst(RX(np.pi / 2, index))\n",
    "\n",
    "                            meas_outcome = \\\n",
    "                                expectation_from_sampling(pyquil_prog + meas_basis_change,\n",
    "                                                          qubits_to_measure,\n",
    "                                                          qc,\n",
    "                                                          samples)\n",
    "\n",
    "                    expectation += term.coefficient * meas_outcome\n",
    "\n",
    "                return expectation.real\n",
    "\n",
    "\n",
    "def parity_even_p(state, marked_qubits):\n",
    "    \"\"\"\n",
    "    Calculates the parity of elements at indexes in marked_qubits\n",
    "    Parity is relative to the binary representation of the integer state.\n",
    "    :param state: The wavefunction index that corresponds to this state.\n",
    "    :param marked_qubits: The indexes to be considered in the parity sum.\n",
    "    :returns: A boolean corresponding to the parity.\n",
    "    \"\"\"\n",
    "    assert isinstance(state, int), \\\n",
    "        f\"{state} is not an integer. Must call parity_even_p with an integer state.\"\n",
    "    mask = 0\n",
    "    for q in marked_qubits:\n",
    "        mask |= 1 << q\n",
    "    return bin(mask & state).count(\"1\") % 2 == 0\n",
    "\n",
    "\n",
    "def expectation_from_sampling(pyquil_program: Program,\n",
    "                              marked_qubits: List[int],\n",
    "                              qc: QuantumComputer,\n",
    "                              samples: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculation of Z_{i} at marked_qubits\n",
    "    Given a wavefunctions, this calculates the expectation value of the Zi\n",
    "    operator where i ranges over all the qubits given in marked_qubits.\n",
    "    :param pyquil_program: pyQuil program generating some state\n",
    "    :param marked_qubits: The qubits within the support of the Z pauli\n",
    "                          operator whose expectation value is being calculated\n",
    "    :param qc: A QuantumComputer object.\n",
    "    :param samples: Number of bitstrings collected to calculate expectation\n",
    "                    from sampling.\n",
    "    :returns: The expectation value as a float.\n",
    "    \"\"\"\n",
    "    program = Program()\n",
    "    ro = program.declare('ro', 'BIT', max(marked_qubits) + 1)\n",
    "    program += pyquil_program\n",
    "    program += [MEASURE(qubit, r) for qubit, r in zip(list(range(max(marked_qubits) + 1)), ro)]\n",
    "    program.wrap_in_numshots_loop(samples)\n",
    "    executable = qc.compile(program)\n",
    "    bitstring_samples = qc.run(executable)\n",
    "    bitstring_tuples = list(map(tuple, bitstring_samples))\n",
    "\n",
    "    freq = Counter(bitstring_tuples)\n",
    "\n",
    "    # perform weighted average\n",
    "    expectation = 0\n",
    "    for bitstring, count in freq.items():\n",
    "        bitstring_int = int(\"\".join([str(x) for x in bitstring[::-1]]), 2)\n",
    "        if parity_even_p(bitstring_int, marked_qubits):\n",
    "            expectation += float(count) / samples\n",
    "        else:\n",
    "            expectation -= float(count) / samples\n",
    "    return expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QOAO Helper Class from grove for the QRBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAOA(object):\n",
    "    def __init__(self, qc: QuantumComputer, qubits: List[int],\n",
    "                 steps: int = 1,\n",
    "                 init_betas: List[float] = None,\n",
    "                 init_gammas: List[float] = None,\n",
    "                 cost_ham: List[PauliSum] = None,\n",
    "                 ref_ham: List[PauliSum]= None,\n",
    "                 driver_ref: Program = None,\n",
    "                 minimizer: ufunc = None,\n",
    "                 minimizer_args: List[Any] = None,\n",
    "                 minimizer_kwargs: Dict[str, Any] = None,\n",
    "                 rand_seed: int = None,\n",
    "                 vqe_options = None,\n",
    "                 store_basis: bool = False):\n",
    "        \"\"\"\n",
    "        QAOA object constructor.\n",
    "        Contains all information for running the QAOA algorithm to find the\n",
    "        ground state of the list of cost clauses.\n",
    "        N.B. This only works if all the terms in the cost Hamiltonian commute with each other.\n",
    "        :param qc: The QuantumComputer connection to use for the algorithm.\n",
    "        :param qubits: The list of qubits to use for the algorithm.\n",
    "        :param steps: The number of mixing and cost function steps to use.\n",
    "                      Default=1.\n",
    "        :param init_betas: Initial values for the beta parameters on the\n",
    "                           mixing terms. Default=None.\n",
    "        :param init_gammas: Initial values for the gamma parameters on the\n",
    "                            cost function. Default=None.\n",
    "        :param cost_ham: list of clauses in the cost function. Must be PauliSum objects.\n",
    "        :param ref_ham: list of clauses in the mixer function. Must be PauliSum objects.\n",
    "        :param driver_ref: The object to define state prep\n",
    "                           for the starting state of the QAOA algorithm.\n",
    "                           Defaults to tensor product of \\|+> states.\n",
    "        :param minimizer: (Optional) Minimization function to pass to the\n",
    "                          Variational-Quantum-Eigensolver method.\n",
    "        :param minimizer_args: (Optional) (list) of additional arguments to pass to the\n",
    "                               minimizer. Default=[].\n",
    "        :param minimizer_kwargs: (Optional) (dict) of optional arguments to pass to\n",
    "                                 the minimizer.  Default={}.\n",
    "        :param rand_seed: integer random seed for initial betas and gammas\n",
    "                          guess.\n",
    "        :param vqe_options: (optional) arguments for VQE run.\n",
    "        :param store_basis: (optional) boolean flag for storing basis states.\n",
    "                            Default=False.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed the random number generator, if a seed is provided.\n",
    "        if rand_seed is not None:\n",
    "            np.random.seed(rand_seed)\n",
    "\n",
    "        # Set attributes values, considering their defaults\n",
    "        self.qc = qc\n",
    "        self.steps = steps\n",
    "        self.qubits = qubits\n",
    "        self.nstates = 2 ** len(qubits)\n",
    "\n",
    "        self.cost_ham = cost_ham or []\n",
    "        self.ref_ham = ref_ham or []\n",
    "\n",
    "        self.minimizer = minimizer or optimize.minimize\n",
    "        self.minimizer_args = minimizer_args or []\n",
    "        self.minimizer_kwargs = minimizer_kwargs or {\n",
    "            'method': 'Nelder-Mead',\n",
    "            'options': {\n",
    "                'disp': True,\n",
    "                'ftol': 1.0e-2,\n",
    "                'xtol': 1.0e-2\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.betas = init_betas or np.random.uniform(0, np.pi, self.steps)[::-1]\n",
    "        self.gammas = init_gammas or np.random.uniform(0, 2*np.pi, self.steps)\n",
    "        self.vqe_options = vqe_options or {}\n",
    "\n",
    "        self.ref_state_prep = (\n",
    "            driver_ref or\n",
    "            Program([H(i) for i in self.qubits])\n",
    "        )\n",
    "\n",
    "        if store_basis:\n",
    "            self.states = [\n",
    "                np.binary_repr(i, width=len(self.qubits))\n",
    "                for i in range(self.nstates)\n",
    "            ]\n",
    "\n",
    "        # Check argument types\n",
    "        if not isinstance(self.cost_ham, (list, tuple)):\n",
    "            raise TypeError(\"cost_ham must be a list of PauliSum objects.\")\n",
    "        if not all([isinstance(x, PauliSum) for x in self.cost_ham]):\n",
    "            raise TypeError(\"cost_ham must be a list of PauliSum objects\")\n",
    "\n",
    "        if not isinstance(self.ref_ham, (list, tuple)):\n",
    "            raise TypeError(\"ref_ham must be a list of PauliSum objects\")\n",
    "        if not all([isinstance(x, PauliSum) for x in self.ref_ham]):\n",
    "            raise TypeError(\"ref_ham must be a list of PauliSum objects\")\n",
    "\n",
    "        if not isinstance(self.ref_state_prep, Program):\n",
    "            raise TypeError(\"Please provide a pyQuil Program object \"\n",
    "                            \"to generate initial state.\")\n",
    "\n",
    "    def get_parameterized_program(self):\n",
    "        \"\"\"\n",
    "        Return a function that accepts parameters and returns a new Quil program.\n",
    "        :returns: a function\n",
    "        \"\"\"\n",
    "        cost_para_programs = []\n",
    "        driver_para_programs = []\n",
    "\n",
    "        for idx in range(self.steps):\n",
    "            cost_list = []\n",
    "            driver_list = []\n",
    "            for cost_pauli_sum in self.cost_ham:\n",
    "                for term in cost_pauli_sum.terms:\n",
    "                    cost_list.append(exponential_map(term))\n",
    "\n",
    "            for driver_pauli_sum in self.ref_ham:\n",
    "                for term in driver_pauli_sum.terms:\n",
    "                    driver_list.append(exponential_map(term))\n",
    "\n",
    "            cost_para_programs.append(cost_list)\n",
    "            driver_para_programs.append(driver_list)\n",
    "\n",
    "        def psi_ref(params):\n",
    "            \"\"\"\n",
    "            Construct a Quil program for the vector (beta, gamma).\n",
    "            :param params: array of 2 . p angles, betas first, then gammas\n",
    "            :return: a pyquil program object\n",
    "            \"\"\"\n",
    "            if len(params) != 2*self.steps:\n",
    "                raise ValueError(\"params doesn't match the number of parameters set by `steps`\")\n",
    "            betas = params[:self.steps]\n",
    "            gammas = params[self.steps:]\n",
    "\n",
    "            prog = Program()\n",
    "            prog += self.ref_state_prep\n",
    "            for idx in range(self.steps):\n",
    "                for fprog in cost_para_programs[idx]:\n",
    "                    prog += fprog(gammas[idx])\n",
    "\n",
    "                for fprog in driver_para_programs[idx]:\n",
    "                    prog += fprog(betas[idx])\n",
    "\n",
    "            return prog\n",
    "\n",
    "        return psi_ref\n",
    "\n",
    "    def get_angles(self) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Finds optimal angles with the quantum variational eigensolver method.\n",
    "        Stored VQE result\n",
    "        :returns: A tuple of the beta angles and the gamma angles for the optimal solution.\n",
    "        \"\"\"\n",
    "        stacked_params = np.hstack((self.betas, self.gammas))\n",
    "        vqe = VQE(self.minimizer, minimizer_args=self.minimizer_args,\n",
    "                  minimizer_kwargs=self.minimizer_kwargs)\n",
    "        cost_ham = reduce(lambda x, y: x + y, self.cost_ham)\n",
    "        # maximizing the cost function!\n",
    "        param_prog = self.get_parameterized_program()\n",
    "        result = vqe.vqe_run(param_prog, cost_ham, stacked_params, qc=self.qc,\n",
    "                             **self.vqe_options)\n",
    "        self.result = result\n",
    "        betas = result.x[:self.steps]\n",
    "        gammas = result.x[self.steps:]\n",
    "        return betas, gammas\n",
    "\n",
    "    def probabilities(self, angles: List[float]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the probability of each state given a particular set of angles.\n",
    "        :param angles: A concatenated list of angles [betas]+[gammas]\n",
    "        :return: The probabilities of each outcome given those angles.\n",
    "        \"\"\"\n",
    "        if isinstance(angles, list):\n",
    "            angles = np.array(angles)\n",
    "\n",
    "        assert angles.shape[0] == 2 * self.steps, \"angles must be 2 * steps\"\n",
    "\n",
    "        param_prog = self.get_parameterized_program()\n",
    "        prog = param_prog(angles)\n",
    "        wf = WavefunctionSimulator().wavefunction(prog)\n",
    "        wf = wf.amplitudes.reshape((-1, 1))\n",
    "        probs = np.zeros_like(wf)\n",
    "        for xx in range(2 ** len(self.qubits)):\n",
    "            probs[xx] = np.conj(wf[xx]) * wf[xx]\n",
    "        return probs\n",
    "\n",
    "    def get_string(self, betas: List[float], gammas: List[float], samples: int = 100):\n",
    "        \"\"\"\n",
    "        Compute the most probable string.\n",
    "        The method assumes you have passed init_betas and init_gammas with your\n",
    "        pre-computed angles or you have run the VQE loop to determine the\n",
    "        angles.  If you have not done this you will be returning the output for\n",
    "        a random set of angles.\n",
    "        :param betas: List of beta angles\n",
    "        :param gammas: List of gamma angles\n",
    "        :param samples: (Optional) number of samples to get back from the QuantumComputer.\n",
    "        :returns: tuple representing the bitstring, Counter object from\n",
    "                  collections holding all output bitstrings and their frequency.\n",
    "        \"\"\"\n",
    "        if samples <= 0 and not isinstance(samples, int):\n",
    "            raise ValueError(\"samples variable must be positive integer\")\n",
    "        param_prog = self.get_parameterized_program()\n",
    "        stacked_params = np.hstack((betas, gammas))\n",
    "\n",
    "        sampling_prog = Program()\n",
    "        ro = sampling_prog.declare('ro', 'BIT', len(self.qubits))\n",
    "        sampling_prog += param_prog(stacked_params)\n",
    "        sampling_prog += [MEASURE(qubit, r) for qubit, r in zip(self.qubits, ro)]\n",
    "        sampling_prog.wrap_in_numshots_loop(samples)\n",
    "        executable = self.qc.compile(sampling_prog)\n",
    "        bitstring_samples = self.qc.run(executable)\n",
    "        bitstring_tuples = list(map(tuple, bitstring_samples))\n",
    "        freq = Counter(bitstring_tuples)\n",
    "        most_frequent_bit_string = max(freq, key=lambda x: freq[x])\n",
    "        return most_frequent_bit_string, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qvm(qvm=None):\n",
    "    if qvm:\n",
    "        return qvm\n",
    "    else:\n",
    "        return api.QVMConnection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Classical Hybrid Restricted Boltzmann Machine Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QBM:\n",
    "    \"\"\"\n",
    "    Quantum Classical Hybrid RBM implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, qvm=None, num_visible=2, num_hidden=1, steps=3, temp=1.0, quant_meas_num=None, bias=False, reduced=False):\n",
    "        # Initializing the Params\n",
    "        self.visible_units = num_visible #Number of visible units\n",
    "        self.hidden_units = num_hidden #Number of hidden units\n",
    "        self.total_units = self.visible_units + self.hidden_units\n",
    "        self.qvm = make_qvm(qvm) #Rigetti QVM Connection Simulator\n",
    "        self.quant_meas_num = quant_meas_num # Number of measuremants to use for Quantum expectation estimation\n",
    "        self.qaoa_steps = steps #Number of steps for QAOA\n",
    "        self.beta_temp = temp #Temperature of the system\n",
    "        self.state_prep_angle = np.arctan(np.exp(-1/self.beta_temp)) * 2.0\n",
    "        self.vqe_inst = VQE(minimizer=minimize, minimizer_kwargs={'method': 'nelder-mead'}) #VQE to minimize the value\n",
    "        self.param_wb = 0.1 * np.sqrt(6. / self.total_units)\n",
    "        self.WEIGHTS = np.asarray(np.random.uniform(low=-self.param_wb, high=self.param_wb, size=(num_visible, num_hidden)))\n",
    "\n",
    "        # Using Bias or not.\n",
    "        if bias:\n",
    "            self.BIAS = np.asarray(np.random.uniform(low=-self.param_wb, high=self.param_wb,size=(self.hidden_units)))\n",
    "        else:\n",
    "            self.BIAS = None\n",
    "\n",
    "        # Using Reduced or Full Botlzman machines.\n",
    "        if reduced:\n",
    "            self.reduced = True\n",
    "        else:\n",
    "            self.reduced = False\n",
    "        \n",
    "    def make_clamped_QAOA(self, data_point, iter):\n",
    "        \"\"\"\n",
    "        Building Quantum Approximate Optimization Algorithm circuit to get the Restricted Boltzmann Machine expectation using a clamped approach\n",
    "        \"\"\"\n",
    "        visible_indices = [i for i in range(self.visible_units)]\n",
    "        hidden_indices = [i + self.visible_units for i in range(self.hidden_units)]\n",
    "        total_indices = [i for i in range(self.total_units)]\n",
    "        # Partial Mixer and Partial Cost Hamiltonian\n",
    "        partial_mixer_operator = []\n",
    "        for i in hidden_indices:\n",
    "            partial_mixer_operator.append(PauliSum([PauliTerm(\"X\", i, 1.0)]))\n",
    "        partial_cost_operator = []\n",
    "        for i in visible_indices:\n",
    "            for j in hidden_indices:\n",
    "                partial_cost_operator.append(PauliSum([PauliTerm(\n",
    "                    \"Z\", i, -1.0 * self.WEIGHTS[i][j - self.visible_units]) * PauliTerm(\"Z\", j, 1.0)]))\n",
    "        if self.BIAS is not None:\n",
    "            for i in hidden_indices:\n",
    "                partial_cost_operator.append(\n",
    "                    PauliSum([PauliTerm(\"Z\", i, -1.0 * self.BIAS[i - self.visible_units])]))\n",
    "        state_prep = pq.Program()\n",
    "        for i, j in enumerate(data_point):\n",
    "            if j == 1:\n",
    "                state_prep += X(i)\n",
    "\n",
    "        for i in hidden_indices:\n",
    "            tmp = pq.Program()\n",
    "            tmp.inst(RX(self.state_prep_angle, i + self.total_units),\n",
    "                     CNOT(i + self.total_units, i))\n",
    "            state_prep += tmp\n",
    "        partial_QAOA = QAOA(qc=self.qvm,\n",
    "                   qubits=total_indices,\n",
    "                   steps=self.qaoa_steps,\n",
    "                   ref_ham=partial_mixer_operator,\n",
    "                   cost_ham=partial_cost_operator,\n",
    "                   driver_ref=state_prep,\n",
    "                   store_basis=True,\n",
    "                   minimizer=fmin_bfgs,\n",
    "                   minimizer_kwargs={'maxiter': 100//iter},\n",
    "                   vqe_options={'samples': self.quant_meas_num},\n",
    "                    rand_seed=1234)\n",
    "                    \n",
    "        nus, gammas = partial_QAOA.get_angles()\n",
    "        program = partial_QAOA.get_parameterized_program()\n",
    "        return nus, gammas, program, 1\n",
    "\n",
    "    def make_unclamped_QAOA(self):\n",
    "        \"\"\"\n",
    "        Building Quantum Approximate Optimization Algorithm circuit to get the Restricted Boltzmann Machine expectation using an unclamped approach.\n",
    "        \"\"\"\n",
    "        visible_indices = [i for i in range(self.visible_units)]\n",
    "        hidden_indices = [i + self.visible_units for i in range(self.hidden_units)]\n",
    "        total_indices = [i for i in range(self.total_units)]\n",
    "        # Full Mixer and Cost Hamiltonian Operator\n",
    "        full_mixer_operator = []\n",
    "        for i in total_indices:\n",
    "            full_mixer_operator.append(PauliSum([PauliTerm(\"X\", i, 1.0)]))\n",
    "        full_cost_operator = []\n",
    "        for i in visible_indices:\n",
    "            for j in hidden_indices:\n",
    "                full_cost_operator.append(PauliSum([PauliTerm(\n",
    "                    \"Z\", i, -1.0 * self.WEIGHTS[i][j - self.visible_units]) * PauliTerm(\"Z\", j, 1.0)]))\n",
    "        if self.BIAS is not None:\n",
    "            for i in hidden_indices:\n",
    "                print(i, self.visible_units, i-self.visible_units, self.BIAS[i-self.visible_units])\n",
    "                full_cost_operator.append(\n",
    "                    PauliSum([PauliTerm(\"Z\", i, -1.0 * self.BIAS[i-self.visible_units])]))\n",
    "\n",
    "        # Prepare all the units in a thermal state of the full mixer hamiltonian.         \n",
    "        state_prep = pq.Program()\n",
    "        for i in total_indices:\n",
    "            tmp = pq.Program()\n",
    "            tmp.inst(RX(self.state_prep_angle, i + self.total_units), CNOT(i + self.total_units, i))\n",
    "            state_prep += tmp\n",
    "        # QAOA on full mixer and full cost hamiltonian evolution\n",
    "        full_QAOA = QAOA(self.qvm,\n",
    "                   qubits=total_indices,\n",
    "                   steps=self.qaoa_steps,\n",
    "                   ref_ham=full_mixer_operator,\n",
    "                   cost_ham=full_cost_operator,\n",
    "                   driver_ref=state_prep,\n",
    "                   store_basis=True,\n",
    "                   minimizer=fmin_bfgs,\n",
    "                   minimizer_kwargs={'maxiter': 100},\n",
    "                   vqe_options={'samples': self.quant_meas_num},\n",
    "                    rand_seed=1234)\n",
    "\n",
    "        nus, gammas = full_QAOA.get_angles()\n",
    "        program = full_QAOA.get_parameterized_program()\n",
    "        return nus, gammas, program, 0\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Returns the sigmoid function required for training.\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Training Parameters for the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = [[1, 1, -1, -1], [1, 1, -1, -1], [-1, -1, 1, 1], [-1, -1, 1, 1]]\n",
    "n_epochs= 10\n",
    "quantum_percentage= 0.7\n",
    "classical_percentage = 0.3\n",
    "qvm = api.QVMConnection()\n",
    "\n",
    "Qalg = QBM(qvm, num_visible=4, num_hidden=1, quant_meas_num=None, bias=False, reduced=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for the Quantum Restrictive Boltzmann Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.030299\n",
      "         Iterations: 41\n",
      "         Function evaluations: 308\n",
      "         Gradient evaluations: 44\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.006074\n",
      "         Iterations: 10\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 13\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.006074\n",
      "         Iterations: 10\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 13\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.006074\n",
      "         Iterations: 7\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 14\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.006074\n",
      "         Iterations: 7\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 14\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  1\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.077008\n",
      "         Iterations: 16\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 17\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.010643\n",
      "         Iterations: 6\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 9\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.010643\n",
      "         Iterations: 6\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 9\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.010643\n",
      "         Iterations: 6\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 15\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.010643\n",
      "         Iterations: 6\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 15\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  2\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.142379\n",
      "         Iterations: 17\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 21\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.013711\n",
      "         Iterations: 8\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 9\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.013711\n",
      "         Iterations: 8\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 9\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.013711\n",
      "         Iterations: 5\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.013711\n",
      "         Iterations: 5\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  3\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.227256\n",
      "         Iterations: 15\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 21\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.014859\n",
      "         Iterations: 12\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 14\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.014859\n",
      "         Iterations: 12\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 14\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.014859\n",
      "         Iterations: 8\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.014859\n",
      "         Iterations: 8\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  4\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.332001\n",
      "         Iterations: 16\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 24\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.014005\n",
      "         Iterations: 21\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 24\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.014005\n",
      "         Iterations: 21\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 24\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.014005\n",
      "         Iterations: 8\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.014005\n",
      "         Iterations: 8\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  5\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.468595\n",
      "         Iterations: 23\n",
      "         Function evaluations: 217\n",
      "         Gradient evaluations: 31\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.011100\n",
      "         Iterations: 9\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 12\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.011100\n",
      "         Iterations: 9\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 12\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.011100\n",
      "         Iterations: 9\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.011100\n",
      "         Iterations: 9\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  6\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.616695\n",
      "         Iterations: 26\n",
      "         Function evaluations: 266\n",
      "         Gradient evaluations: 38\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.005343\n",
      "         Iterations: 9\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.005343\n",
      "         Iterations: 9\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.005343\n",
      "         Iterations: 6\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.005343\n",
      "         Iterations: 6\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  7\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.769519\n",
      "         Iterations: 10\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 18\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.001654\n",
      "         Iterations: 8\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 13\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.001654\n",
      "         Iterations: 8\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 13\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.001654\n",
      "         Iterations: 11\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 15\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.001654\n",
      "         Iterations: 11\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 15\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  8\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.672661\n",
      "         Iterations: 34\n",
      "         Function evaluations: 343\n",
      "         Gradient evaluations: 49\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.007588\n",
      "         Iterations: 6\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.007588\n",
      "         Iterations: 6\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.007588\n",
      "         Iterations: 8\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.007588\n",
      "         Iterations: 8\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 11\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "Epoch:  9\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.556802\n",
      "         Iterations: 14\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 22\n",
      "Found model expectation program\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.012987\n",
      "         Iterations: 7\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.012987\n",
      "         Iterations: 7\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.012987\n",
      "         Iterations: 8\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "WARNING: Fast method for expectation will be used. Noise\n",
      "                     models will be ineffective\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.012987\n",
      "         Iterations: 8\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 10\n",
      "Found Model Expectation\n",
      "None\n",
      "Training Done\n",
      "The Weights Determined by the algorithm are\n",
      "[[-0.49922696]\n",
      " [-0.41204369]\n",
      " [ 0.48445726]\n",
      " [ 0.45398332]]\n"
     ]
    }
   ],
   "source": [
    "assert(quantum_percentage+ classical_percentage == 1.0)\n",
    "DATA = np.asarray(DATA)\n",
    "assert(len(DATA[0]) <= Qalg.visible_units)\n",
    "\n",
    "#Training\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    visible_indices = [i for i in range(Qalg.visible_units)]\n",
    "    hidden_indices = [i + Qalg.visible_units for i in range(Qalg.hidden_units)]\n",
    "    total_indices = [i for i in range(Qalg.total_units)]\n",
    "\n",
    "    new_weights = deepcopy(Qalg.WEIGHTS)\n",
    "    if Qalg.BIAS is not None:\n",
    "        new_bias = deepcopy(Qalg.BIAS)\n",
    "    unc_nus, unc_gammas, unc_para_prog, _ = Qalg.make_unclamped_QAOA()\n",
    "    unc_mod_samp_prog = unc_para_prog(np.hstack((unc_nus, unc_gammas)))\n",
    "\n",
    "    print('Found model expectation program')\n",
    "\n",
    "    unc_neg_phase_quant = np.zeros_like(Qalg.WEIGHTS)\n",
    "\n",
    "    for i in range(Qalg.visible_units):\n",
    "        for j in range(Qalg.hidden_units):\n",
    "            model_expectation = Qalg.vqe_inst.expectation(unc_mod_samp_prog, sZ(visible_indices[i]) * sZ(hidden_indices[j]), Qalg.quant_meas_num, Qalg.qvm)\n",
    "            unc_neg_phase_quant[i][j] = model_expectation\n",
    "            \n",
    "    unc_neg_phase_quant  *= (1. / float(len(DATA)))\n",
    "\n",
    "    if Qalg.BIAS is not None:\n",
    "        unc_neg_phase_quant_bias = np.zeros_like(Qalg.BIAS)\n",
    "        for i in range(Qalg.hidden_units):\n",
    "            model_expectation = Qalg.vqe_inst.expectation(unc_mod_samp_prog, sZ(hidden_indices[i]), Qalg.quant_meas_num, Qalg.qvm)\n",
    "            unc_neg_phase_quant_bias[i] = model_expectation\n",
    "        unc_neg_phase_quant_bias *= (1. / float(len(DATA)))\n",
    "\n",
    "    pos_hidden_probs = Qalg.sigmoid(np.dot(DATA, Qalg.WEIGHTS))\n",
    "    pos_hidden_states = pos_hidden_probs > np.random.rand(len(DATA), Qalg.hidden_units)\n",
    "    pos_phase_classical = np.dot(DATA.T, pos_hidden_probs) * 1./len(DATA)\n",
    "    c_pos_phase_quant = np.zeros_like(Qalg.WEIGHTS)\n",
    "\n",
    "    if Qalg.BIAS is not None:\n",
    "        c_pos_phase_quant_bias = np.zeros_like(Qalg.BIAS)\n",
    "\n",
    "    if not Qalg.reduced:\n",
    "        iter_dat= len(DATA)\n",
    "        for data in DATA:\n",
    "            c_nus, c_gammas, c_para_prog, _ = Qalg.make_clamped_QAOA(data_point=data, iter= iter_dat)\n",
    "            c_mod_samp_prog = c_para_prog(np.hstack((c_nus, c_gammas)))\n",
    "\n",
    "            print('Found Model Expectation')\n",
    "\n",
    "            ct_pos_phase_quant = np.zeros_like(Qalg.WEIGHTS)\n",
    "\n",
    "            for i in range(Qalg.visible_units):\n",
    "                for j in range(Qalg.hidden_units):\n",
    "                    model_expectation = Qalg.vqe_inst.expectation(c_mod_samp_prog,sZ(visible_indices[i]) * sZ(hidden_indices[j]), Qalg.quant_meas_num, Qalg.qvm)\n",
    "                    ct_pos_phase_quant[i][j] = model_expectation\n",
    "            c_pos_phase_quant += ct_pos_phase_quant\n",
    "\n",
    "            if Qalg.BIAS is not None:\n",
    "                ct_pos_phase_quant_bias = np.zeros_like(Qalg.BIAS)\n",
    "                for i in range(Qalg.hidden_units):\n",
    "                    model_expectation = Qalg.vqe_inst.expectation(c_mod_samp_prog, sZ(hidden_indices[j]), Qalg.quant_meas_num, Qalg.qvm)\n",
    "                    ct_pos_phase_quant_bias[i] = model_expectation\n",
    "                c_pos_phase_quant_bias += ct_pos_phase_quant_bias\n",
    "                \n",
    "        c_pos_phase_quant *= (1. / float(len(DATA)))\n",
    "        if Qalg.BIAS is not None:\n",
    "            c_pos_phase_quant_bias *= (1. / float(len(DATA)))\n",
    "\n",
    "    learning_rate=0.1\n",
    "    neg_visible_activations = np.dot(pos_hidden_states, Qalg.WEIGHTS.T)\n",
    "    neg_visible_probs = Qalg.sigmoid(neg_visible_activations)\n",
    "    neg_hidden_activations = np.dot(neg_visible_probs, Qalg.WEIGHTS)\n",
    "    neg_hidden_probs = Qalg.sigmoid(neg_hidden_activations)\n",
    "    neg_phase_classical = np.dot(neg_visible_probs.T, neg_hidden_probs) * 1./len(DATA)\n",
    "    new_weights += learning_rate * (classical_percentage * (pos_phase_classical - neg_phase_classical) + quantum_percentage * (c_pos_phase_quant - unc_neg_phase_quant))\n",
    "\n",
    "    print(Qalg.BIAS)\n",
    "\n",
    "    Qalg.WEIGHTS = deepcopy(new_weights)\n",
    "\n",
    "    if Qalg.BIAS is not None:\n",
    "        Qalg.BIAS = deepcopy(new_bias)\n",
    "        print(Qalg.BIAS)\n",
    "\n",
    "    print('Training Done')\n",
    "\n",
    "\n",
    "print(\"The Weights Determined by the algorithm are\")\n",
    "print(Qalg.WEIGHTS)\n",
    "\n",
    "if Qalg.BIAS is not None:\n",
    "    print(Qalg.BIAS)\n",
    "\n",
    "# To save the discovered data\n",
    "with open(\"RBM_info.txt\", \"w\") as f: \n",
    "    np.savetxt(f,Qalg.WEIGHTS)\n",
    "    if Qalg.BIAS is not None:\n",
    "        np.savetxt(f,Qalg.BIAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual results are: \n",
      " \n",
      "\n",
      "[[0.1359068]\n",
      " [0.1359068]\n",
      " [0.8640932]\n",
      " [0.8640932]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual results are: \\n \\n\")\n",
    "\n",
    "def transform(DATA):\n",
    "    return Qalg.sigmoid(np.dot(DATA, Qalg.WEIGHTS))\n",
    "\n",
    "print(transform(DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1268aaaa6abb12edf0150eea36efa234bf2c9ace401cebf260491e328d4ffe4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
